0. Create a page where all logins and keys are stored. Upload project to github and make so that the password file is not uploaded.
1. Fix any missing fields in player crawling and fields that need to be added to db
2. Save files to hard drive instead of directly uploading to DB (just to be sure if something fails)
3. Add merge page where logic happens for player and tournament parsing, change some fields from crawling to merge (address, name parsing)
4. Crawl players and add to database
5. Check any mistakes/errors after all players crawled and rerun files if fixable in merge
6. Add crawling for tournaments
7. Add parsing for tournaments
8. Check any mistakes/errors after all tournaments crawled and rerun files if fixable in merge
9. Create cronjobs and automate the process
10. Create API for the data (mongoDB + python + Flask)
11. Create webpage where to show pdga data
12. Learn react and make the data searchable through filters and/or remade statistics.
13. Share work on fb/linkedin


# Add tests?
# What kind of tests should there be?
# Did the crawl work?
# Did correct data come out of the crawling?
# Was the file saved correctly?
# Was the data parsed at merge correctly?
# Was the data sent to the database?


# MongoDb next steps
# Parsing process for player data
# Crawl couple for db
# Create index for PDGA number
# Create process to update player data
# Crawl all players and add to mongodb

# A file where the file is opened and looped. Select if player or tournament
# A file where the actual parsing happens. One for player and 1 for tournament
# Send data also in the in the file where data file is opened.



    #run_player_crawl.sh management command for cronjob
    #run_player_parse.sh management command for cronjob
    #run_tournament_crawl.sh management command for cronjob
    #run_tournament_parse.sh management command for cronjob

    #run_player_crawl.py file that calls player crawler
    #run_tournament_crawl.py file that calls tournament crawler
    #run_tournament_parse.py file that calls tournament parser
    #run_player_parse.py file that calls player parser

    #crawl_player.py player info crawler
    #crawl_tournament.py tournament info crawler

    #tournament.py
    #player.py

    #helpers_data_management.py
    #helpers_data_parsing.py
    #pdga_find_latest_id.py
    #schemas.py
    #connect_mongodb.py
    #find_file.py


#Management command executes parse file
#Parse file calls player.py
#Function is given a single json dict
#parses or updates mongo db


#Fields which are always available even if player inactive
  #Name
  #pdga number
  #location
  #classification
  #member since
  #membership status
  #membership status expiration date
  #career events
  #career wins
  #career earnings

#Fields from crawler
    #full_name
    #pdga_number
    #location_full
    #classification
    #member since
    #current rating
    #rating_difference
    #total_events
    #total_wins
    #career earnings
    #individual_tournament_years
    #first_crawl_date
#Fields that need to created at parsing
    #Created from player_name
        #first_name
        #last_name
    #Created from player_location_raw
        #city
        #state
        #country
    #Created from player_membership_status
        #membership_status (True/False, add field info depending on what is received from player_membership_status)
        #membership (add if normal/eagle club/ace club/birdie club and so on)
    #Created from player_current_rating
        #highest_rating (crawl from pdga and add logic that would do this from current rating)
        #lowest_rating (crawl from pdga and add logic that would do this from current rating)
    #Created from player_rating_updated
        #latest_rating_update (need to parse date)
    #Created from player_certified_status
        #certified_status (parse from crawler data)
    #Created from player_certified_status_expiration
        #certified_status_expiration_date (need to parse date)
    #Created from player_id
        #pdga_page_link (create from ID)
    #Created from Tournaments
        #played_event_ids (add data to this field once tournaments parsed)
        #played_countries (add data to this field once tournaments parsed)
    #Created during parse, no original data source
        #latest_update (add during parse)
        #fields_updated (add if field exists already, logic how to compare data)

# How to work around data that should not be updated?
  # Solution 1
  # Parse everything normally
  # Check requirements which field should be deleted and which not
  # del <Variable>

  # Solution 2
  # Different parsing logic for each variable.



################################################################

Create individual functions for each field that parse the data.
Add US state parsing to location as a lot of players have the state in 2 letter code
Create logic how to parse names with multiple middle names/honorifics examples, 671, 817, 859, 957, 44138, 44139
Add a field for middle name
Add field to Player for gender and crawl it from https://www.pdga.com/players/stats
Add field to Player for age (estimate) that is parsed from tournament divisions
